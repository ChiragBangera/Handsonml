{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch gradient descent implementation with early stopping for softmax regression\n",
    "iris = load_iris(as_frame=True)\n",
    "X = iris.data[['petal length (cm)', 'petal width (cm)']].values\n",
    "y = iris.target.values\n",
    "X_with_bias_term = np.insert(X, 0, 1, axis=1)\n",
    "# np.c_[np.ones(len(X)), X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.2\n",
    "validation_ratio = 0.2\n",
    "total_size = len(X_with_bias_term)\n",
    "\n",
    "test_size = int(test_ratio * total_size)\n",
    "validation_size = int(validation_ratio * total_size)\n",
    "train_size = total_size - test_size - validation_size\n",
    "\n",
    "np.random.seed(42)\n",
    "rnd_indices = np.random.permutation(total_size)\n",
    "\n",
    "X_train = X_with_bias_term[rnd_indices[:train_size]]\n",
    "X_test = X_with_bias_term[rnd_indices[-test_size:]]\n",
    "y_train = y[rnd_indices[:train_size]]\n",
    "y_test = y[rnd_indices[-test_size:]]\n",
    "X_valid = X_with_bias_term[rnd_indices[train_size:-test_size]]\n",
    "y_valid = y[rnd_indices[train_size:-test_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onehot encoder manual\n",
    "\n",
    "def to_one_hot(y):\n",
    "    return np.diag(np.ones(y.max() + 1))[y]\n",
    "\n",
    "y_train_one_hot = to_one_hot(y_train)\n",
    "y_test_one_hot = to_one_hot(y_test)\n",
    "y_valid_one_hot = to_one_hot(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the inputs by calculation mean and std\n",
    "mean = X_train[:, 1:].mean(axis=0)\n",
    "std = X_train[:, 1:].std(axis=0)\n",
    "X_train[:, 1:] = (X_train[:, 1:] - mean)/std\n",
    "X_test[:, 1:] = (X_test[:, 1:] - mean)/std\n",
    "X_valid[:, 1:] = (X_valid[:, 1:] - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing softmax fucntion\n",
    "\n",
    "def softmax(logits):\n",
    "    exps = np.exp(logits)\n",
    "    exp_sums = exps.sum(axis = 1, keepdims = True)\n",
    "    return exps/exp_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining number of inputs and outputs\n",
    "n_inputs = X_train.shape[1]\n",
    "n_outputs = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = .5\n",
    "epoch = 5001\n",
    "m = len(X_train)\n",
    "epsilon = 1e5\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(n_inputs, n_outputs) # selecting random theta valus\n",
    "\n",
    "for _ in range(epoch):\n",
    "    logits = X_train @ theta\n",
    "    y_proba = softmax(logits)\n",
    "    if epoch % 1000 == 0:\n",
    "        y_proba_valid = softmax(X_valid @ theta)\n",
    "        xenthropy_losses = -(y_valid_one_hot * np.log(y_proba_valid + epsilon))\n",
    "    error = y_proba - y_train_one_hot\n",
    "    gradients = 1 / m * X_train.T @ error\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making prediction on the validation set\n",
    "\n",
    "logits = X_valid @ theta\n",
    "y_proba = softmax(logits)\n",
    "y_predict = y_proba.argmax(axis = 1)\n",
    "\n",
    "accuracy_score = (y_predict == y_valid).mean()\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding l2 regularization \n",
    "\n",
    "eta = .5\n",
    "epoch = 5001\n",
    "m = len(X_train)\n",
    "epsilon = 1e5\n",
    "alpha = .01\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(n_inputs, n_outputs) # selecting random theta valus\n",
    "\n",
    "for _ in range(epoch):\n",
    "    logits = X_train @ theta\n",
    "    y_proba = softmax(logits)\n",
    "    if epoch % 1000 == 0:\n",
    "        y_proba_valid = softmax(X_valid @ theta)\n",
    "        xenthropy_losses = -(y_valid_one_hot * np.log(y_proba_valid + epsilon))\n",
    "        l2_loss = 1/2 * (theta[1:] ** 2)/sum()\n",
    "        total_loss = xenthropy_losses.sum(axis=1).mean() + alpha @ l2_loss\n",
    "    error = y_proba - y_train_one_hot\n",
    "    gradients = 1 / m * X_train.T @ error\n",
    "    gradients += np.r_[np.zeros([1, n_outputs]), alpha * theta[1:]]\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = X_valid @ theta\n",
    "y_proba = softmax(logits)\n",
    "y_predict = y_proba.argmax(axis=1)\n",
    "\n",
    "accuracy_score = (y_predict == y_valid).mean()\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "islp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
