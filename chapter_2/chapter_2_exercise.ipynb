{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature  as cfeature\n",
    "import geopandas as gpd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV,\\\n",
    "    RandomizedSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder,OneHotEncoder,\\\n",
    "    MinMaxScaler,StandardScaler, FunctionTransformer\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.compose import TransformedTargetRegressor,make_column_selector,make_column_transformer,ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_array, check_is_fitted\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from scipy.stats import binom\n",
    "from sklearn import set_config\n",
    "from sklearn.metrics import r2_score,root_mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import randint, expon, loguniform\n",
    "from scipy import stats\n",
    "import joblib\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.base import MetaEstimatorMixin, clone\n",
    "from sklearn.utils.estimator_checks import check_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chira\\AppData\\Local\\Temp\\ipykernel_3308\\1834649392.py:8: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  housing_tarball.extractall(path='datasets')             # extract the data into the checked folder\n"
     ]
    }
   ],
   "source": [
    "def load_housing_data():\n",
    "    tarball_path = Path('datasets/housing.tgz')\n",
    "    if not tarball_path.is_file():\n",
    "        Path('datasets').mkdir(parents=True, exist_ok=True)\n",
    "        url = 'https://github.com/ageron/data/raw/main/housing.tgz' # datapatH\n",
    "        urllib.request.urlretrieve(url,tarball_path)                # gets the data and puts it in the cheked path\n",
    "        with tarfile.open(tarball_path) as housing_tarball:         # read the data the data from the checked path\n",
    "            housing_tarball.extractall(path='datasets')             # extract the data into the checked folder\n",
    "    return pd.read_csv(Path('datasets/housing/housing.csv'))        # pd read the file \n",
    "\n",
    "housing  = load_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "\n",
    "housing['income_cats'] = pd.cut(housing['median_income'], bins = [0,1.5,3,4.5,6,np.inf], labels=[1,2,3,4,5])\n",
    "housing['combined'] =housing.apply(lambda x: '_'.join(x.values.astype(str)), axis=1)\n",
    "\n",
    "htrain,htest = train_test_split(housing, test_size=.2,stratify=housing['income_cats'], random_state=42)\n",
    "\n",
    "for set_ in (htrain, htest):\n",
    "    set_.drop(['combined', 'income_cats'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = htrain.drop('median_house_value',axis=1)\n",
    "housing_label = htrain['median_house_value'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.98, 0.  , 0.  , 0.  , 0.  , 0.13, 0.55, 0.  , 0.56],\n",
       "       [0.64, 0.  , 0.11, 0.04, 0.  , 0.  , 0.  , 0.  , 0.99, 0.  ],\n",
       "       [0.  , 0.65, 0.  , 0.  , 0.01, 0.  , 0.49, 0.59, 0.  , 0.28]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom Transformers\n",
    "class ClusterSimilarity(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" The fit method identifies clusters using KMeans clustering and the transform method checks \n",
    "    for similiratity between a data point and against all the clusters identified by the KMeans clustering fit method\n",
    "    \n",
    "    Args:\n",
    "        BaseEstimator (_type_): _description_\n",
    "        TransformerMixin (_type_): _description_\n",
    "    \"\"\"\n",
    "    def __init__(self, n_clusters = 10, gamma = .1, random_state= None) -> None:\n",
    "        self.n_clusters = n_clusters\n",
    "        self.gamma = gamma\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X, y = None, sample_weight = None):\n",
    "        self.kmeans_ = KMeans(self.n_clusters, random_state=self.random_state)\n",
    "        self.kmeans_.fit(X, sample_weight=sample_weight)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\n",
    "    \n",
    "    def get_feature_names_out(self, names = None):\n",
    "            return [f'Cluster {i} similarity' for i in range(self.n_clusters)]\n",
    "        \n",
    "## using custom tranformer\n",
    "cluster_simil = ClusterSimilarity(n_clusters = 10, gamma=1., random_state=42)\n",
    "similarity = cluster_simil.fit_transform(housing[['latitude','longitude']],sample_weight = housing_label)\n",
    "\n",
    "similarity[:3].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16512, 24)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline building\n",
    "\n",
    "def num_pipeline():\n",
    "    return make_pipeline(\n",
    "        SimpleImputer(strategy='median'),\n",
    "        StandardScaler()\n",
    ")\n",
    "\n",
    "def cat_pipeline():\n",
    "    return make_pipeline(\n",
    "        SimpleImputer(strategy='most_frequent'),\n",
    "        OneHotEncoder(handle_unknown='ignore')\n",
    ")\n",
    "\n",
    "def column_ratio(X):\n",
    "    return X[:,[0]]/X[:,[1]]\n",
    "\n",
    "def ratio_name(function_tranformer, feature_names_in):\n",
    "    return ['ratio'] #feature names out\n",
    "\n",
    "def ratio_pipeline():\n",
    "    return make_pipeline(\n",
    "        SimpleImputer(strategy='median'),\n",
    "        FunctionTransformer(column_ratio, feature_names_out=ratio_name),\n",
    "        StandardScaler()\n",
    "    )\n",
    "    \n",
    "def log_pipeline():\n",
    "    return make_pipeline(\n",
    "        SimpleImputer(strategy='median'),\n",
    "        FunctionTransformer(np.log, feature_names_out='one-to-one',inverse_func=np.exp),\n",
    "        StandardScaler()\n",
    "    )\n",
    "    \n",
    "def cluster_simil_pipeline():\n",
    "    cluster_simil = ClusterSimilarity(n_clusters=10, gamma = 1, random_state=42)\n",
    "    return cluster_simil\n",
    "    \n",
    "preprocessing = ColumnTransformer([\n",
    "    ('bedroom',ratio_pipeline(), ['total_bedrooms', 'total_rooms']),\n",
    "    ('rooms_per_house',ratio_pipeline(), ['total_rooms', 'households']),\n",
    "    ('people_per_house',ratio_pipeline(), ['population', 'households']),\n",
    "    ('log',log_pipeline(), ['total_bedrooms', 'total_rooms', 'population','households','median_income']),\n",
    "    ('geo',cluster_simil_pipeline(), ['latitude', 'longitude']),\n",
    "    ('cat',cat_pipeline(), make_column_selector(dtype_include=object)),\n",
    "    ],\n",
    "                                  remainder=num_pipeline()\n",
    ")\n",
    "\n",
    "\n",
    "housing_prepared = preprocessing.fit_transform(housing)\n",
    "housing_prepared.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 custom transformer\n",
    "\n",
    "class FeatureFromRegressor(MetaEstimatorMixin, BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, estimator) -> None:\n",
    "        self.estimator = estimator\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        estimator_ = clone(self.estimator) #creates a clone of the original estimator to ensure the original estimator is not modified\n",
    "        estimator_.fit(X, y) # fits the cloned estimator with input features X and target value y\n",
    "        self.estimator_ = estimator_ # stores the fitted estimator in the class instsance\n",
    "        self.n_features_in_ = self.estimator_.n_features_in_ # stores the number of features used by the fitted estimator\n",
    "        if hasattr(self.estimator, 'feature_names_in'): # checks if the estimator has a 'feature_names_in_' attribute\n",
    "            self.feature_names_in_ = self.estimator.feature_names_in_ # if it has the variable it stores in the instance variable\n",
    "        return self # always returns self\n",
    "        \n",
    "    \n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self) # checks if the transformer id fitted before calling transform\n",
    "        predictions = self.estimator_.predict(X) # uses the fitted estimator to make predictions based on input featuers X\n",
    "        if predictions.ndim == 1:  # checks if the predictions are 1-dimensional array (which can happen if the there is only one dimensional array)\n",
    "            predictions = predictions.reshape(-1,1) # if  1D reshape the array\n",
    "        return predictions  # return the predictions, which are now treated as new features to be used in subsequent steps of the pipeline\n",
    "    \n",
    "    \n",
    "    def get_feature_names_out(self, names=None): \n",
    "        check_is_fitted(self)\n",
    "        n_outputs = getattr(self.estimator_, 'n_outputs_', 1) #Retrieves the number of outputs from the fitted estimator using 'n_outputs' attribute\n",
    "        estimator_class_name = self.estimator_.__class__.__name__ #Gets the class name of the estimator\n",
    "        estimator_short_name = estimator_class_name.lower().replace('_', '') # Converts the class name into lower case and removes underscores to create a standardized name for the estimator\n",
    "        return [f'{estimator_short_name}_prediction_{i}' for i in range(n_outputs)]\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_estimator(FeatureFromRegressor(KNeighborsRegressor()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[486100.66666667],\n",
       "       [435250.        ],\n",
       "       [105100.        ],\n",
       "       ...,\n",
       "       [148800.        ],\n",
       "       [500001.        ],\n",
       "       [234333.33333333]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_reg = KNeighborsRegressor(n_neighbors=3, weights='distance')\n",
    "knn_transformer = FeatureFromRegressor(knn_reg)\n",
    "geo_features = housing[['latitude', 'longitude']]\n",
    "knn_transformer.fit_transform(geo_features, housing_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kneighborsregressor_prediction_0']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_transformer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = [(name, clone(transformer), columns) for name , transformer, columns in preprocessing.transformers]\n",
    "geo_index = [name for name, _,_ in transformers].index('geo')\n",
    "transformers[geo_index] = ('geo', knn_transformer, ['latitude', 'longitude'])\n",
    "\n",
    "new_geo_preprocessing = ColumnTransformer(transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "islp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
